{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Специализация \"Машинное обучение и анализ данных\"\n",
    "# <center> Проект \"Алиса\":<br>Идентификация пользователей по посещенным веб-страницам\n",
    "## <div style=\"text-align: right\">Автор: Пётр Зюзиков</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исходные данные:\n",
    "В нашем распоряжении есть база данных пользователей и их рабочих сессий в Интернете: последовательностей из нескольких веб-сайтов, посещенных подряд. Данные взяты из [соревнования \"Catch Me If You Can\"](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2).\n",
    "### Задача:\n",
    "Найти наиболее подходящий алгоритм и обучить его на этих данных, чтобы максимально успешно определять, принадлежит ли сессия определённому пользователю (бинарная классификация). Решаем для пользователя Алисы (Alice).\n",
    "**Метрика: ROC AUC**\n",
    "### Определение успеха:\n",
    "1) Написать стабильно работающий код модели.<br>\n",
    "2) Улучшить метрику в сравнении с эталонным решением.<br>\n",
    "Личная цель: Войти в топ 5% лучших решений среди 4000+ студентов за всё время или максимально подойти к этому рубежу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Промежуточные итоги\n",
    "\n",
    "### ROC AUC\n",
    "**Public: 0.95154, cross-validation: 0.00000. Эталон: 0.94928, 0.92647.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://i.imgur.com/bcA7Cvx.png\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Особенности решения:\n",
    "Начну с вещи, которую сознательно не стал делать: leaderboard probing. Из-за текущего формата соревнования доступен только публичный лидерборд и нет возможности узнать итоговые результаты на приватной части тестов. И это открывает широкие возможности для махинаций с файлом ответов. Вместо этого реализована кросс-валидация с правильной зависимостью от времени, и она настолько же важна, как и результаты публичного лидерборда.\n",
    "\n",
    "В эталонном решении значение кросс-валидации и наиболее подходящий гиперпараметр неверны из-за ошибки формата даты в записи данных, что хорошо показано в kaggle notebook [Model validation in a competition - Fixing CV](https://www.kaggle.com/sgdread/model-validation-in-a-competition-fixing-cv). Я добавил исправление этой ошибки в эталонное решение.\n",
    "\n",
    "Исследование на Data Leaks похоже на EDA, поэтому его я с удовольствием провёл. Тренировочные данные даны нам в двух форматах, тестовые в одном. И благодаря пониманию логики преобразования тренировочных данных можно объединить часть тестовых, и выдать для этих объединений более точные прогнозы. Здесь особенно пригодилась библиотека numpy, так как с pandas обработка данных занимала слишком много времени и усложняла тестирование.\n",
    "\n",
    "Добавим в модель новые признаки: особенно нам интересны признаки, которые объединяют время посещения и название сайта. В эталонном решении уже хорошо отобраны признаки, которые учитывают эти показатели независимо."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Шаблон решения\n",
    "Этот исходный код — [от организаторов](https://www.kaggle.com/kashnitsky/alice-template-for-your-solution), с добавленным мной кодом исправления ошибки в данных. Задания курса в течение предыдущих 6 недель включают в себя самостоятельную реализацию кода всех частей этого решения. Ноутбуки c моими решениями можно посмотреть в папке Preparation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building sparse site features] done in 35 s\n",
      "[Building additional features] done in 6 s\n",
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:   17.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score 0.926474775872798\n",
      "[Cross-validation] done in 26 s\n",
      "[Test prediction and submission] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = 'kaggle'\n",
    "AUTHOR = 'EyeShield77'\n",
    "SEED = 17\n",
    "N_JOBS = 4\n",
    "NUM_TIME_SPLITS = 10    # for time-based cross-validation\n",
    "SITE_NGRAMS = (1, 5)    # site ngrams for \"bag of sites\"\n",
    "MAX_FEATURES = 50000    # max features for \"bag of sites\"\n",
    "BEST_LOGIT_C = 5.45559  # precomputed tuned C for logistic regression\n",
    "\n",
    "TIMES = ['time%s' % i for i in range(1, 11)]\n",
    "SITES = ['site%s' % i for i in range(1, 11)]\n",
    "\n",
    "# reporting running times\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "\n",
    "# fixing data parsing mistake\n",
    "def fix_incorrect_date_formats(df, columns_to_fix):\n",
    "    for time in columns_to_fix:\n",
    "        d = df[time]\n",
    "        d_fix = d[d.dt.day <= 12]\n",
    "        d_fix = pd.to_datetime(d_fix.apply(str), format='%Y-%d-%m %H:%M:%S')\n",
    "        df.loc[d_fix.index.values, time] = d_fix\n",
    "    return df\n",
    "\n",
    "def prepare_sparse_features(after_load_fn, path_to_train, path_to_test, path_to_site_dict, vectorizer_params):\n",
    "    train_df = pd.read_csv(path_to_train,\n",
    "                       index_col='session_id', parse_dates=TIMES)\n",
    "    test_df = pd.read_csv(path_to_test,\n",
    "                      index_col='session_id', parse_dates=TIMES)\n",
    "\n",
    "    # Fixing the dates\n",
    "    train_df = after_load_fn(train_df)\n",
    "    test_df = after_load_fn(test_df)\n",
    "\n",
    "    # Sort the data by time\n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    \n",
    "    # read site -> id mapping provided by competition organizers \n",
    "    with open(path_to_site_dict, 'rb') as f:\n",
    "        site2id = pickle.load(f)\n",
    "    # create an inverse id _> site mapping\n",
    "    id2site = {v:k for (k, v) in site2id.items()}\n",
    "    # we treat site with id 0 as \"unknown\"\n",
    "    id2site[0] = 'unknown'\n",
    "    \n",
    "    # Transform data into format which can be fed into TfidfVectorizer\n",
    "    # This time we prefer to represent sessions with site names, not site ids. \n",
    "    # It's less efficient but thus it'll be more convenient to interpret model weights.\n",
    "    train_sessions = train_df[SITES].fillna(0).astype('int').apply(lambda row:\n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    test_sessions = test_df[SITES].fillna(0).astype('int').apply(lambda row:\n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "    # to be split into 'mail', 'google' and 'com')\n",
    "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "    X_train = vectorizer.fit_transform(train_sessions)\n",
    "    X_test = vectorizer.transform(test_sessions)\n",
    "    y_train = train_df['target'].astype('int').values\n",
    "    \n",
    "    # we'll need site visit times for further feature engineering\n",
    "    train_times, test_times = train_df[TIMES], test_df[TIMES]\n",
    "    \n",
    "    return X_train, X_test, y_train, vectorizer, train_times, test_times\n",
    "\n",
    "\n",
    "def add_features(times, X_sparse):\n",
    "    hour = times['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int').values.reshape(-1, 1)\n",
    "    sess_duration = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[s]')\\\n",
    "\t\t   .astype('int').values.reshape(-1, 1)\n",
    "    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n",
    "    year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) / 1e5\n",
    "\n",
    "    X = hstack([X_sparse, morning, day, evening, night, sess_duration, day_of_week, month, year_month])\n",
    "    return X\n",
    "\n",
    "with timer('Building sparse site features'):\n",
    "    X_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times = \\\n",
    "        prepare_sparse_features(\n",
    "            after_load_fn=(lambda df: fix_incorrect_date_formats(df, TIMES)), # Applying dates fix\n",
    "            path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "            path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "            path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n",
    "            vectorizer_params={'ngram_range': SITE_NGRAMS,\n",
    "                               'max_features': MAX_FEATURES,\n",
    "                               'tokenizer': lambda s: s.split()})\n",
    "\n",
    "with timer('Building additional features'):\n",
    "    X_train_final = add_features(train_times, X_train_sites)\n",
    "    X_test_final = add_features(test_times, X_test_sites)\n",
    "\n",
    "\n",
    "with timer('Cross-validation'):\n",
    "    time_split = TimeSeriesSplit(n_splits=NUM_TIME_SPLITS)\n",
    "    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n",
    "\n",
    "    c_values = [BEST_LOGIT_C]\n",
    "\n",
    "    logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=N_JOBS, cv=time_split, verbose=1)\n",
    "    logit_grid_searcher.fit(X_train_final, y_train)\n",
    "    print('CV score', logit_grid_searcher.best_score_)\n",
    "\n",
    "\n",
    "with timer('Test prediction and submission'):\n",
    "    test_pred = logit_grid_searcher.predict_proba(X_test_final)[:, 1]\n",
    "    pred_df = pd.DataFrame(test_pred, index=np.arange(1, test_pred.shape[0] + 1),\n",
    "                       columns=['target'])\n",
    "    pred_df.to_csv(f'submission_alice_{AUTHOR}.csv', index_label='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведём подбор гиперпараметров: очень вероятно, что лучший параметр, подобранный организаторами, не подходит для исправленного датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:   13.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.9269516972229194\n",
      "Best logit C: {'C': 3}\n",
      "[Cross-validation] done in 22 s\n",
      "[Test prediction and submission] done in 0 s\n"
     ]
    }
   ],
   "source": [
    "with timer('Cross-validation'):\n",
    "    time_split = TimeSeriesSplit(n_splits=NUM_TIME_SPLITS)\n",
    "    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n",
    "\n",
    "    #c_values = np.logspace(-2, 2, 20)\n",
    "    #c_values = np.linspae(2, 6, 41)\n",
    "    c_values = [3]\n",
    "    \n",
    "    logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=N_JOBS, cv=time_split, verbose=1)\n",
    "    logit_grid_searcher.fit(X_train_final, y_train)\n",
    "    print('CV score:', logit_grid_searcher.best_score_)\n",
    "    print('Best logit C:', logit_grid_searcher.best_params_)\n",
    "\n",
    "\n",
    "with timer('Test prediction and submission'):\n",
    "    test_pred = logit_grid_searcher.predict_proba(X_test_final)[:, 1]\n",
    "    pred_df = pd.DataFrame(test_pred, index=np.arange(1, test_pred.shape[0] + 1),\n",
    "                       columns=['target'])\n",
    "    pred_df.to_csv(f'submission_alice_{AUTHOR}2.csv', index_label='session_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, наш новый ориентир: 0.94998 (СV — 0.92695)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA: Исследуем данные, придумываем новые признаки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Считаем данные [соревнования](https://inclass.kaggle.com/c/catch-me-if-you-can-intruder-detection-through-webpage-session-tracking2) в DataFrame train_df и test_df (обучающая и тестовая выборки) и посмотрим на них.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_df.shape = (253561, 21)\n",
      "test_df.shape = (82797, 20)\n",
      "Разделение на классы в тренировочной выборке:\n",
      "Элис: 2297 , остальные: 251264\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_DATA = 'kaggle/'\n",
    "TIMES = ['time%d' % i for i in range(1, 11)]\n",
    "SITES = ['site%d' % i for i in range(1, 11)]\n",
    "train_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "                       index_col='session_id', parse_dates=TIMES)\n",
    "test_df = pd.read_csv(os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "                      index_col='session_id', parse_dates=TIMES)\n",
    "print('train_df.shape =', train_df.shape)\n",
    "print('test_df.shape =', test_df.shape)\n",
    "print('Разделение на классы в тренировочной выборке:')\n",
    "print('Элис:', train_df['target'].value_counts()[1], ', остальные:', train_df['target'].value_counts()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>...</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>718</td>\n",
       "      <td>2014-02-20 10:02:45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>890</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:50</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>941.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>942.0</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>...</td>\n",
       "      <td>2014-02-22 11:19:51</td>\n",
       "      <td>3847.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>3846.0</td>\n",
       "      <td>2014-02-22 11:19:52</td>\n",
       "      <td>1516.0</td>\n",
       "      <td>2014-02-22 11:20:15</td>\n",
       "      <td>1518.0</td>\n",
       "      <td>2014-02-22 11:20:16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>14769</td>\n",
       "      <td>2013-12-16 16:40:17</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2013-12-16 16:40:18</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14769.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>...</td>\n",
       "      <td>2013-12-16 16:40:19</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:20</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:21</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:22</td>\n",
       "      <td>14768.0</td>\n",
       "      <td>2013-12-16 16:40:24</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2    site3  \\\n",
       "session_id                                                                  \n",
       "1             718 2014-02-20 10:02:45    NaN                 NaT      NaN   \n",
       "2             890 2014-02-22 11:19:50  941.0 2014-02-22 11:19:50   3847.0   \n",
       "3           14769 2013-12-16 16:40:17   39.0 2013-12-16 16:40:18  14768.0   \n",
       "\n",
       "                         time3    site4               time4  site5  \\\n",
       "session_id                                                           \n",
       "1                          NaT      NaN                 NaT    NaN   \n",
       "2          2014-02-22 11:19:51    941.0 2014-02-22 11:19:51  942.0   \n",
       "3          2013-12-16 16:40:19  14769.0 2013-12-16 16:40:19   37.0   \n",
       "\n",
       "                         time5  ...               time6    site7  \\\n",
       "session_id                      ...                                \n",
       "1                          NaT  ...                 NaT      NaN   \n",
       "2          2014-02-22 11:19:51  ... 2014-02-22 11:19:51   3847.0   \n",
       "3          2013-12-16 16:40:19  ... 2013-12-16 16:40:19  14768.0   \n",
       "\n",
       "                         time7    site8               time8    site9  \\\n",
       "session_id                                                             \n",
       "1                          NaT      NaN                 NaT      NaN   \n",
       "2          2014-02-22 11:19:52   3846.0 2014-02-22 11:19:52   1516.0   \n",
       "3          2013-12-16 16:40:20  14768.0 2013-12-16 16:40:21  14768.0   \n",
       "\n",
       "                         time9   site10              time10 target  \n",
       "session_id                                                          \n",
       "1                          NaT      NaN                 NaT      0  \n",
       "2          2014-02-22 11:20:15   1518.0 2014-02-22 11:20:16      0  \n",
       "3          2013-12-16 16:40:22  14768.0 2013-12-16 16:40:24      0  \n",
       "\n",
       "[3 rows x 21 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>site1</th>\n",
       "      <th>time1</th>\n",
       "      <th>site2</th>\n",
       "      <th>time2</th>\n",
       "      <th>site3</th>\n",
       "      <th>time3</th>\n",
       "      <th>site4</th>\n",
       "      <th>time4</th>\n",
       "      <th>site5</th>\n",
       "      <th>time5</th>\n",
       "      <th>site6</th>\n",
       "      <th>time6</th>\n",
       "      <th>site7</th>\n",
       "      <th>time7</th>\n",
       "      <th>site8</th>\n",
       "      <th>time8</th>\n",
       "      <th>site9</th>\n",
       "      <th>time9</th>\n",
       "      <th>site10</th>\n",
       "      <th>time10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>session_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>35.0</td>\n",
       "      <td>2014-10-04 11:19:53</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>23.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>2211.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2014-10-04 11:19:54</td>\n",
       "      <td>44582.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "      <td>15336.0</td>\n",
       "      <td>2014-10-04 11:20:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>782</td>\n",
       "      <td>2014-07-03 11:00:28</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:53</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:00:58</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:06</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:09</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:10</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:23</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:29</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:30</td>\n",
       "      <td>782.0</td>\n",
       "      <td>2014-07-03 11:01:53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>2014-12-05 15:55:12</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:13</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:55:14</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:15</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:16</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:17</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:18</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2014-12-05 15:56:19</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:33</td>\n",
       "      <td>1445.0</td>\n",
       "      <td>2014-12-05 15:56:36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            site1               time1  site2               time2  site3  \\\n",
       "session_id                                                                \n",
       "1              29 2014-10-04 11:19:53   35.0 2014-10-04 11:19:53   22.0   \n",
       "2             782 2014-07-03 11:00:28  782.0 2014-07-03 11:00:53  782.0   \n",
       "3              55 2014-12-05 15:55:12   55.0 2014-12-05 15:55:13   55.0   \n",
       "\n",
       "                         time3  site4               time4  site5  \\\n",
       "session_id                                                         \n",
       "1          2014-10-04 11:19:54  321.0 2014-10-04 11:19:54   23.0   \n",
       "2          2014-07-03 11:00:58  782.0 2014-07-03 11:01:06  782.0   \n",
       "3          2014-12-05 15:55:14   55.0 2014-12-05 15:56:15   55.0   \n",
       "\n",
       "                         time5   site6               time6   site7  \\\n",
       "session_id                                                           \n",
       "1          2014-10-04 11:19:54  2211.0 2014-10-04 11:19:54  6730.0   \n",
       "2          2014-07-03 11:01:09   782.0 2014-07-03 11:01:10   782.0   \n",
       "3          2014-12-05 15:56:16    55.0 2014-12-05 15:56:17    55.0   \n",
       "\n",
       "                         time7  site8               time8    site9  \\\n",
       "session_id                                                           \n",
       "1          2014-10-04 11:19:54   21.0 2014-10-04 11:19:54  44582.0   \n",
       "2          2014-07-03 11:01:23  782.0 2014-07-03 11:01:29    782.0   \n",
       "3          2014-12-05 15:56:18   55.0 2014-12-05 15:56:19   1445.0   \n",
       "\n",
       "                         time9   site10              time10  \n",
       "session_id                                                   \n",
       "1          2014-10-04 11:20:00  15336.0 2014-10-04 11:20:00  \n",
       "2          2014-07-03 11:01:30    782.0 2014-07-03 11:01:53  \n",
       "3          2014-12-05 15:56:33   1445.0 2014-12-05 15:56:36  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В выборках мы видим следующие признаки:\n",
    "    - site1 – индекс первого посещенного сайта в сессии\n",
    "    - time1 – время посещения первого сайта в сессии\n",
    "    - ...\n",
    "    - site10 – индекс 10-го посещенного сайта в сессии\n",
    "    - time10 – время посещения 10-го сайта в сессии\n",
    "    - user_id – ID пользователя\n",
    "    \n",
    "Сессии пользователей выделены таким образом, что они не могут быть длинее получаса или 10 сайтов. То есть сессия считается оконченной либо когда пользователь посетил 10 сайтов подряд, либо когда сессия заняла по времени более 30 минут."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Настоящие исходные данные.** Но это не все данные, которые нам доступны. Есть ещё архив train.zip с сессиями каждого пользователя в формате timestamp & site: время и посещённый сайт. И именно эти данные предобработали в тренировочную выборку, и по такому же алгоритму тестовые данные обработали в тестовую.\n",
    "Сессии делили по полные по 10 сайтов до момента, пока не появлялся пропуск более получаса. На этом моменте записывали \"короткую\" сессию из 1-10 сайтов и NaN на остальных местах.\n",
    "\n",
    "**Также давайте вспомним, как люди в 2013-2014 году выходят в Интернет.** Если кратко, то с компьютера ровно так же, как и сейчас: запускается браузер с окнами, которые остались с предыдущей сессии. В примере снизу на примере данных Элис мы видим, что за 5 секунд она открыла 9 сайтов. Есть и альтернативная гипотеза: приличная часть обращений не является открытой новой вкладкой в браузере, это переадресации или другие автоматические действия браузера/открытых сайтов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество записей: 22769\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>site</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2013-02-12 16:32:24</td>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>www.info-jeunes.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2013-02-12 16:32:25</td>\n",
       "      <td>www.google.fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2013-02-12 16:32:26</td>\n",
       "      <td>www.info-jeunes.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>platform.twitter.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>www.info-jeunes.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2013-02-12 16:32:27</td>\n",
       "      <td>www.facebook.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2013-02-12 16:32:28</td>\n",
       "      <td>www.info-jeunes.net</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2013-02-12 16:32:29</td>\n",
       "      <td>twitter.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp                  site\n",
       "4  2013-02-12 16:32:24         www.google.fr\n",
       "5  2013-02-12 16:32:25   www.info-jeunes.net\n",
       "6  2013-02-12 16:32:25         www.google.fr\n",
       "7  2013-02-12 16:32:26   www.info-jeunes.net\n",
       "8  2013-02-12 16:32:27  platform.twitter.com\n",
       "9  2013-02-12 16:32:27   www.info-jeunes.net\n",
       "10 2013-02-12 16:32:27      www.facebook.com\n",
       "11 2013-02-12 16:32:28   www.info-jeunes.net\n",
       "12 2013-02-12 16:32:29           twitter.com"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alice_df = pd.read_csv('kaggle/Alice_log.csv', parse_dates=['timestamp'])\n",
    "print(f'Количество записей: {alice_df.shape[0]}')\n",
    "alice_df[4:13]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, сколько на самом деле было получасовых промежутков между открытиями сайтов у Элис. А также 20, 15, 10, 5, 3, 2 и 1-минутных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 min: 34\n",
      "20 min: 39\n",
      "15 min: 48\n",
      "10 min: 62\n",
      " 5 min: 100\n",
      " 3 min: 155\n",
      " 2 min: 220\n",
      " 1 min: 406\n"
     ]
    }
   ],
   "source": [
    "alice_diff = (np.diff(alice_df['timestamp'])/np.timedelta64(1, 's'))\n",
    "for i in [30, 20, 15, 10, 5, 3, 2, 1]:\n",
    "    print(f'{i:2.0f} min: {len(alice_diff[alice_diff > 60*i])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какое количество сайтов в каждой сессии Алисы, которые она успевает посмотреть до 5-минутного перерыва?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 553,    2,   46,    3,  372, 3691,   71,    2,  100,  113,  133,\n",
       "        297,  211,    2,  153,    5,  101,   75,   65,  759,   38,   14,\n",
       "        945,   10, 1417,    1, 1046,    1,    4,    1,   96,  179,  349,\n",
       "         56,    2,   87,   21,    6,   23,    1,  286,   48,   66,   21,\n",
       "        108,    2,   56,   25,    1,   95,    2,  732,   51,  733,   39,\n",
       "          1,  455,  124,    5,  648,  113,    2,   23,   46,    1,   99,\n",
       "        162,   28,   14,    1,  442,   99,    1,   36,    1,  191,    1,\n",
       "         52,   35, 2692,    3,  219,  278,  469,  212,  117,    5,   28,\n",
       "        121,   99,  955,   17,  725,  450,    1,    4,    2,  457,    3],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intervals = np.diff(np.where(alice_diff > 60*5))[0]\n",
    "intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что когда мы видим в ответе на предыдущий вопрос числа типа 3691 или любые другие больше нескольких сотен, то должны понимать, что на приличную часть сайтов Алиса не заходит сознательно, это автоматический процесс. Посмотрим, какие сайты и насколько часто открываются первыми после перерыва хотя бы в 20 секунд: скорее всего, эти сайты Алиса открывает осознанно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count, id, site name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[158, 21, 'www.google.fr'],\n",
       " [58, 80, 's.youtube.com'],\n",
       " [53, 37, 'twitter.com'],\n",
       " [50, 3000, 'vk.com'],\n",
       " [40, 77, 'i1.ytimg.com'],\n",
       " [39, 270, 'api.bing.com'],\n",
       " [38, 335, 'www.dailymotion.com'],\n",
       " [33, 733, 'translate.google.fr'],\n",
       " [33, 76, 'www.youtube.com'],\n",
       " [30, 677, 'drive.google.com'],\n",
       " [30, 81, 'r4---sn-gxo5uxg-jqbe.googlevideo.com'],\n",
       " [25, 616, 'docs.google.com'],\n",
       " [22, 27221, 'apiv1.scribblelive.com'],\n",
       " [17, 1919, 'syndication.twitter.com'],\n",
       " [17, 29, 'www.facebook.com'],\n",
       " [16, 879, 'r1---sn-gxo5uxg-jqbe.googlevideo.com'],\n",
       " [16, 82, 'r2---sn-gxo5uxg-jqbe.googlevideo.com'],\n",
       " [16, 22, 'apis.google.com'],\n",
       " [14, 17283, 'www.demotivateur.fr'],\n",
       " [14, 5302, 'login.vk.com'],\n",
       " [14, 229, 'clients1.google.fr'],\n",
       " [14, 63, 'ieonline.microsoft.com'],\n",
       " [13, 7832, 'www.info-jeunes.net']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SECONDS_BETWEEN_SESSIONS = 20\n",
    "\n",
    "intervals = np.diff(np.where(alice_diff >= SECONDS_BETWEEN_SESSIONS))[0]\n",
    "site_freq = []\n",
    "idx = 2\n",
    "for i, td in enumerate(intervals):\n",
    "    idx += td\n",
    "    site_freq.append(alice_df['site'].iloc[idx])\n",
    "uniq, counts = np.unique(site_freq, return_counts=True)\n",
    "path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl')\n",
    "with open(path_to_site_dict, 'rb') as f:\n",
    "    site2id = pickle.load(f)\n",
    "sites_of_interest = sorted([[counts[i], site2id[uniq[i]], uniq[i]] for i in np.where(counts >= 13)[0]], reverse=True)\n",
    "print('Count, id, site name')\n",
    "sites_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итак, это будет одним из наших признаков: есть ли перерыв, и является ли открытый сайт после этого перерыва одним из сайтов выше.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Есть ли такая же закономерность в сайтах, которыми Алиса заканчивает сессию? Если на сайте заканчивается сессия, это может означать, что Алиса осталась на нём сидеть."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count, id, site name\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[46, 3000, 'vk.com'],\n",
       " [26, 21, 'www.google.fr'],\n",
       " [19, 677, 'drive.google.com'],\n",
       " [10, 63, 'ieonline.microsoft.com']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SECONDS_BETWEEN_SESSIONS = 120\n",
    "\n",
    "intervals = np.diff(np.where(alice_diff > SECONDS_BETWEEN_SESSIONS))[0]\n",
    "site_freq = []\n",
    "idx = 2\n",
    "for td in intervals:\n",
    "    idx += td\n",
    "#    site_freq.append('.'.join(alice_df['site'].iloc[idx-1].split('.')[-2:]))\n",
    "    site_freq.append(alice_df['site'].iloc[idx-1])\n",
    "uniq, counts = np.unique(site_freq, return_counts=True)\n",
    "sites_of_interest = sorted([[counts[i], site2id[uniq[i]], uniq[i]] for i in np.where(counts >= 10)[0]], reverse=True)\n",
    "print('Count, id, site name')\n",
    "sites_of_interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обоих группах значительно выделяются vk.com и сервисы google."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Нам интересно добавление признаков, сочетающих комбинацию условий по времени и имени сайтов: в эталонном решении таких признаков нет.** Там используются признаки отдельно по времени и отдельно по комбинации открываемых сайтов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем больше id сайта, тем реже на него заходили другие пользователи. Наиболее интересными для нас являются комбинации не очень часто посещаемых сайтов общей популяцией, но на которые достаточно часто заходит Алиса. Посещения остальных сайтов отловят признаки TfidfVectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Собираем итоговое решение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обновлённая функция подготовки данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMES = ['time%s' % i for i in range(1, 11)]\n",
    "SITES = ['site%s' % i for i in range(1, 11)]\n",
    "\n",
    "def prepare_sparse_features(after_load_fn, path_to_train, path_to_test, path_to_site_dict, vectorizer_params):\n",
    "    train_df = pd.read_csv(path_to_train,\n",
    "                       index_col='session_id', parse_dates=TIMES)\n",
    "    test_df = pd.read_csv(path_to_test,\n",
    "                      index_col='session_id', parse_dates=TIMES)\n",
    "\n",
    "    # Fixing the dates\n",
    "    train_df = after_load_fn(train_df)\n",
    "    test_df = after_load_fn(test_df)\n",
    "\n",
    "    # Sort the data by time\n",
    "    train_df = train_df.sort_values(by='time1')\n",
    "    \n",
    "    # read site -> id mapping provided by competition organizers \n",
    "    with open(path_to_site_dict, 'rb') as f:\n",
    "        site2id = pickle.load(f)\n",
    "    # create an inverse id _> site mapping\n",
    "    id2site = {v:k for (k, v) in site2id.items()}\n",
    "    # we treat site with id 0 as \"unknown\"\n",
    "    id2site[0] = 'unknown'\n",
    "    \n",
    "    # we'll need site visit times for further feature engineering\n",
    "    train_times, test_times = train_df[TIMES], test_df[TIMES]\n",
    "    \n",
    "    # Transform data into format which can be fed into TfidfVectorizer\n",
    "    # This time we prefer to represent sessions with site names, not site ids. \n",
    "    # It's less efficient but thus it'll be more convenient to interpret model weights.\n",
    "    train_sessions = train_df[SITES].fillna(0).astype('int').apply(lambda row:\n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    test_sessions = test_df[SITES].fillna(0).astype('int').apply(lambda row:\n",
    "                                                     ' '.join([id2site[i] for i in row]), axis=1).tolist()\n",
    "    \n",
    "    #Combining short sessions with full ones\n",
    "    test_df = test_df.sort_values(by='time1')\n",
    "    indices = test_df.index.tolist()\n",
    "    ttest = np.array(test_df[TIMES])\n",
    "    rows_to_skip = []\n",
    "    row_value_to_insert = []\n",
    "    res = 0\n",
    "    for i, t1 in enumerate(ttest[1:, 0], start=1):\n",
    "        if np.isnat(ttest[i, 9]):\n",
    "            for j in range(1, min(i+1, 11)):\n",
    "                if i-j not in rows_to_skip and 0 <= (t1-ttest[i-j, 9])/np.timedelta64(1, 's') <= 2:\n",
    "                    res += 1\n",
    "                    test_sessions[indices[i-j]-1] += ' ' \\\n",
    "                            + ' '.join([word for word in test_sessions[indices[i]-1].split() if word != 'unknown'])\n",
    "                    test_sessions[indices[i]-1] = test_sessions[indices[i-j]-1]\n",
    "                    rows_to_skip.append(indices[i]-1)\n",
    "                    row_value_to_insert.append(indices[i-j]-1)\n",
    "                    break\n",
    "    #print('test =', res)\n",
    "    \n",
    "    # we'll tell TfidfVectorizer that we'd like to split data by whitespaces only \n",
    "    # so that it doesn't split by dots (we wouldn't like to have 'mail.google.com' \n",
    "    # to be split into 'mail', 'google' and 'com')\n",
    "    vectorizer = TfidfVectorizer(**vectorizer_params)\n",
    "    X_train = vectorizer.fit_transform(train_sessions)\n",
    "    X_test = vectorizer.transform(test_sessions)\n",
    "    y_train = train_df['target'].astype('int').values\n",
    "    \n",
    "    # we'll need sessions with site ids for future engineering\n",
    "    train_sessions_ids = train_df[SITES].fillna(0).astype('int').apply(lambda row: list(row), axis=1).tolist()\n",
    "    test_sessions_ids = test_df[SITES].fillna(0).astype('int').apply(lambda row: list(row), axis=1).tolist()\n",
    "    \n",
    "    return X_train, X_test, y_train, vectorizer, train_times, test_times, train_sessions_ids, test_sessions_ids, \\\n",
    "            rows_to_skip, row_value_to_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building sparse site features] done in 60 s\n"
     ]
    }
   ],
   "source": [
    "with timer('Building sparse site features'):\n",
    "    X_train_sites, X_test_sites, y_train, vectorizer, train_times, test_times, train_sessions, test_sessions, rows1, rows2 = \\\n",
    "        prepare_sparse_features(\n",
    "            after_load_fn=(lambda df: fix_incorrect_date_formats(df, TIMES)), # Applying dates fix\n",
    "            path_to_train=os.path.join(PATH_TO_DATA, 'train_sessions.csv'),\n",
    "            path_to_test=os.path.join(PATH_TO_DATA, 'test_sessions.csv'),\n",
    "            path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl'),\n",
    "            vectorizer_params={'ngram_range': SITE_NGRAMS,\n",
    "                               'max_features': MAX_FEATURES,\n",
    "                               'tokenizer': lambda s: s.split()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка структур для создания новых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_BETWEEN_SESSIONS = 20\n",
    "\n",
    "intervals = np.diff(np.where(alice_diff >= SECONDS_BETWEEN_SESSIONS))[0]\n",
    "site_freq, site_freq2 = [], []\n",
    "idx = 2\n",
    "for i, td in enumerate(intervals):\n",
    "    idx += td\n",
    "    site_freq.append(alice_df['site'].iloc[idx])\n",
    "    if i < len(intervals)-1 and intervals[i+1] > 1:\n",
    "        site_freq2.append(alice_df['site'].iloc[idx+1])\n",
    "uniq, counts = np.unique(site_freq, return_counts=True)\n",
    "path_to_site_dict=os.path.join(PATH_TO_DATA, 'site_dic.pkl')\n",
    "with open(path_to_site_dict, 'rb') as f:\n",
    "    site2id = pickle.load(f)\n",
    "id2site = {v:k for (k, v) in site2id.items()}\n",
    "uniq = [site2id[site] for site in uniq]\n",
    "sites_of_interest = [uniq[i] for i in np.where(counts >= 10)[0] if uniq[i] >= 500]\n",
    "alice_sites = set([site2id[site] for site in alice_df['site']])\n",
    "uniq = set(uniq).union(set([site2id[site] for site in site_freq2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обновлённая функция по добавлению новых признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_features(times, sessions, X_sparse, sites_of_interest, uniq, alice_sites):\n",
    "    hour = times['time1'].apply(lambda ts: ts.hour)\n",
    "    morning = ((hour >= 7) & (hour <= 11)).astype('int').values.reshape(-1, 1)\n",
    "    day = ((hour >= 12) & (hour <= 18)).astype('int').values.reshape(-1, 1)\n",
    "    evening = ((hour >= 19) & (hour <= 23)).astype('int').values.reshape(-1, 1)\n",
    "    night = ((hour >= 0) & (hour <= 6)).astype('int').values.reshape(-1, 1)\n",
    "#     sess_duration = (times.max(axis=1) - times.min(axis=1)).astype('timedelta64[s]')\\\n",
    "# \t\t   .astype('int').values.reshape(-1, 1)\n",
    "    day_of_week = times['time1'].apply(lambda t: t.weekday()).values.reshape(-1, 1)\n",
    "    month = times['time1'].apply(lambda t: t.month).values.reshape(-1, 1) \n",
    "    year_month = times['time1'].apply(lambda t: 100 * t.year + t.month).values.reshape(-1, 1) / 1e5\n",
    "    \n",
    "    times = np.array(times)\n",
    "    new_features = np.zeros((len(times), len(sites_of_interest)+1), int)\n",
    "    for i, session in enumerate(sessions):\n",
    "        for j, tdiff in enumerate(np.diff(times[i, :]), start=1):\n",
    "            if tdiff/np.timedelta64(1, 's') >= SECONDS_BETWEEN_SESSIONS:\n",
    "                if session[j] not in uniq and session[j] in alice_sites:\n",
    "                    new_features[i, -1] = 1\n",
    "                if session[j] in sites_of_interest:\n",
    "                    new_features[i, sites_of_interest.index(session[j])] = 1\n",
    "    X = hstack([X_sparse, morning, day, evening, night, day_of_week, month, year_month, new_features])\n",
    "#     X = hstack([X_sparse, morning, day, evening, night, day_of_week, month, year_month])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Building additional features] done in 18 s\n"
     ]
    }
   ],
   "source": [
    "with timer('Building additional features'):\n",
    "    X_train_final = add_features(train_times, train_sessions, X_train_sites, sites_of_interest, uniq, alice_sites)\n",
    "    X_test_final = add_features(test_times, test_sessions, X_test_sites, sites_of_interest, uniq, alice_sites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассчёт кросс-валидации и подбор гиперпараметра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 21 candidates, totalling 210 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   30.3s\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=4)]: Done 210 out of 210 | elapsed:  2.9min finished\n"
     ]
    }
   ],
   "source": [
    "with timer('Cross-validation'):\n",
    "    time_split = TimeSeriesSplit(n_splits=NUM_TIME_SPLITS)\n",
    "    logit = LogisticRegression(random_state=SEED, solver='liblinear')\n",
    "\n",
    "    c_values = np.linspace(2, 4, 21)\n",
    "\n",
    "    logit_grid_searcher = GridSearchCV(estimator=logit, param_grid={'C': c_values},\n",
    "                                  scoring='roc_auc', n_jobs=N_JOBS, cv=time_split, verbose=1)\n",
    "    logit_grid_searcher.fit(X_train_final, y_train)\n",
    "    print('CV score', logit_grid_searcher.best_score_)\n",
    "    print('Best logit C:', logit_grid_searcher.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Создание submission файла и его правка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with timer('Test prediction and submission'):\n",
    "    test_pred = logit_grid_searcher.predict_proba(X_test_final)[:, 1]\n",
    "    pred_df = pd.DataFrame(test_pred, index=np.arange(1, test_pred.shape[0] + 1),\n",
    "                       columns=['target'])\n",
    "    pred_df.to_csv(f'submission_alice_{AUTHOR}2.csv', index_label='session_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = pd.read_csv('submission_alice_EyeShield77_2.csv', header=0, index_col='session_id')\n",
    "ans = np.array(answers['target'])\n",
    "for i, j in zip(rows1, rows2):\n",
    "    answers['target'].iloc[i] = answers['target'].iloc[j]\n",
    "answers.to_csv(f'submission_alice_EyeShield77_upd', index_label='session_id')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
